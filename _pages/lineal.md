---
layout: page
title: Algebra Lineal y Machine Learning
---


Bienvenido al maravilloso mundo del machine learning! Antes de ponernos comodos veamos algunos ejemplos incomodos donde se nos manifiesta la imperiosa necesidad de usar elementos del algebra lineal. La idea de este articulo es motivar con ejemplos concretos el por que necesitamos entender y conocer sobre: vectores, matrices, tranformaciones lineales, etc...



* [Datasets](#datasets)
* [Imagenes y video](#imagenes-y-video)
* [Variables categoricas](#variables-categoricas)
* [Regresion Lineal](#regresion-Lineal)
* [Regularizacion](#regularizacion)
* [PCA y dimensionalidad](#pca-y-dimensionalidad)
* [Text-mining](#text-mining)
* [Sistemas de recomendacion](#sistemas-de-recomendacion)
* [Inteligencia artificial](#inteligencia-artificial)


### Datasets 

La idea central de machine learning, es ajustar un modelo a un set de datos (dataset) de manera de poder predecir alguna variable (target) en funcion de otras variables explicativas (features). Para esto es necesario ordenar los datos en una tabla donde cada fila represente una observacion y que cada columna sea una feature que represente nuestra obvservacion.


Por ejemplo, imaginemos que queremos entrenar un modelo que nos permite predecir las ventas de un producto en funcion de la inversion que se hace en publicidad, nuestras columnas de caracteristicas seran los 3 tipos de medio de comunicacion. Television, radio y periodicos seran las features y Sales el target. Cada observacion sera un mes de los gastos. El dataset original tiene el registro de los ultimos 200 meses, aca vemos solo los ultimos 4:

```
TV,Radio,Newspaper,Sales
230.1,37.8,69.2,22.1
44.5,39.3,45.1,10.4
17.2,45.9,69.3,9.3
151.5,41.3,58.5,18.5
```

Esto en realidad es una **matriz**, una estructura basica del algebra lineal. Es mas, cuando uno parte el data set en features y target, lo que hace es tener una _matriz_ (X) y un  a **vector** target (y). Un vector es otro elemento fundamental del algebra lineal.


Cada final tiene la misma longitud, esto es el mismo numero de columnas, por lo tanto decimos la que data fue vectoriazada y cada observacion nueva sera un vector o varios que serviran para predecir su variable target.


### Imagenes y video

Quizas uno ya trabaje con imagenes o este intersado en computer vision (machine learning para imagenes), para esto cada imagen puede ser pensada como una matriz (ancho x alto) donde cada lugar representa un pixel, y el valor que sera asignado tiene que ver con el color. Si la imagen fuera en blanco y negro, el valor sera en escala de grises. Pero si fuera una imagen en colores, en realidad tendriamos 3 matrices, una para cada color (RGB).

![](https://github.com/muydipalma/home/raw/v3/0.png)

![](https://github.com/muydipalma/home/raw/v3/1.png)

```
[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
[1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]
[1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]
[1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1]
```

Asi, una imagen no es mas que otro ejemplo de el uso de una _matriz_. Entonces cualquier operacion que querramos hacer con imagenes (cropping, scaling, shearing), necesariamente nos obliga a conocer la notacion y las operaciones con matrices.

Y un video? Un video, por ejemplo uno que tenga 30 fotogramas por segundo. Podemos pensarlo como una sucesion de matrices, donde cada fotograma es una matriz, esta sucesion de matrices la llamaremos **tensor**. 

### Variables categoricas

A veces tenemos sistemas donde algunas variables pueden tomar valores categoricos. Imaginen un dataset con clientes de un supermercado por ejemplo:

```

Genero   E.Civil   Ingresos  Sucursal   Categoria   Units   Ganancia 
   F          S      $50K       CABA   Snack Foods      5     27.38 
   M          M      $90K        CBA    Vegetables      5     14.90 
   F          M      $70K       CABA   Snack Foods      3      5.52
   M          M      $50K        MDQ         Candy      4      4.54
   F          M      $70K       CABA   Snack Foods      3      5.52
   M          M      $50K        MDQ         Candy      4      6.44
   F          M      $50K        MDQ    Vegetables      4      6.24

``` 

En este caso uno podria querer predecir la variable Ganancia en funcion del genero, sucursal, categoria etc!

Para poder aplicar cualquier tecnica de machine learning es necesario que las featues sean numericas, no 'caba, cba o mdq' como en el ejemplo. La solucion mas comun a esto se llama 'one hot encoding'. Donde se representa a cada columna de variables categoricas como una tabla. En esta nueva tabla, cada valor posible de las categorias tiene asociada una columa con valores numericos.

Por ejemplo, para la columna sucursal (asumiendo que solo son 3 posibles sucursales) usamos la siguiente representacion:

```
CABA, CBA, MDQ
1, 0, 0
0, 1, 0
1, 0, 0
0, 0, 1
1, 0, 0
0, 0, 1
0, 0, 1
...
```

Cada fila ahora es un **vector** binario, donde solo hay un 1 que corresponde a la categoaria a la que pertenece. Esto es un ejemplo de **sparse representation**, todo un tema del algebra lineal. Esto se debera hacer para cada columna categorica y terminaremos con una matriz con mas columnas de las que empezamos pero conteniendo la misma informacion.

### Regresion Lineal

Mas alla de la representacion de los datos, veamos un ejemplo mas concreto de machine learning. La regresion lineal es una de las herramientas mas simples para modelar la dependencia entre variables.

Imgenen que tenemos los siguientes datos:

![img0](https://github.com/muydipalma/home/raw/v3/fig0.png)

Lo primero que una observa es que hay una relacion lineal entre x e y, entonces intentaria postular lo siguiente:

```
y = m . x + b

```

Existe una pendiente _m_ y una ordenada al origen _b_ que definen una recta que pasa por todos los puntos? Como encuentro _m_ y _b_ ?


Si esto es verdad, entonces todos los puntos

```
( x, y )
(10, 41)
(20, 32)
(30, 35)
(40, 25)
(50, 22)
(60, 18)
```

Deberian cumplir con la ecuacion de esta recta:
 
```
41  = m . 10  +  b
32  = m . 20  +  b
35  = m . 30  +  b
25  = m . 40  +  b
22  = m . 50  +  b
18  = m . 60  +  b

```

Antes de intentar despejar _m_ y _b_ retoquemos un poco los datos:

Primero metemos un 1 multiplicando.

```
41  = m . 10  +  b . 1
32  = m . 20  +  b . 1
35  = m . 30  +  b . 1
25  = m . 40  +  b . 1
22  = m . 50  +  b . 1
18  = m . 60  +  b . 1
```
Luego cambiamos de nombre:

```
41  = b1 . 10  +  b0 . 1
32  = b1 . 20  +  b0 . 1
35  = b1 . 30  +  b0 . 1
25  = b1 . 40  +  b0 . 1
22  = b1 . 50  +  b0 . 1
18  = b1 . 60  +  b0 . 1
```

Ahora usando notacion matricial podemos rescribir esto como un producto de un vector por una matriz:


<img src="https://latex.codecogs.com/png.latex?\begin{bmatrix}41\\32\\35\\25\\18\\\end{bmatrix}=\begin{bmatrix}1&10\\1&20\\1&30\\1&40\\1&50\\1&60\end{bmatrix}\cdot\begin{bmatrix}b_0\\b_1\end{bmatrix}">

Podemos resolver este sistema de ecuaciones para despejar m y b? Si reescribimos esto en notacion matricial nos queda:
    
<img src="https://latex.codecogs.com/png.latex?\large\hat{y}=X\cdot\hat{b}">


Necesitamos saber de operaciones matriciales para poder "despejar" nuestro vectores de coeficientes! Lo que uno pensaria que es pasar "dividiendo" la matriz X es en realidad encontrar la inversa de esta matriz. Por otro lado, imaginemos que tenemos 10,000 puntos y 200 variables.. nuestra notacion sigue sirviendo, ahora X sera una matriz de 10000x200 y los vectores _x_ e _y_ 10,000x1.



### Regularizacion

Tomemos el ultimo ejemplo, donde tenemos datos de 10,000 personas y para cada una el valor de 200 variables. Imaginemos que queremos predecir una de ellas en funcion de las otras 199. 

<img src="https://latex.codecogs.com/png.latex?\large\hat{y}=\hat{x}_0%20\cdot%20b_0%20+\hat{x}_1%20\cdot%20b_1%20+\hat{x}_2%20\cdot%20b_2%20+...+\hat{x}_{199}%20\cdot%20b_{199}">


Como vimos recien podriamos hacer una regresion lineal para encontrar esos 199 coeficientes, pero que pasa si en realidad solo 4 de esas 199 fuera realmente variables explicativas y el resto solo fuera informacion relevante o redundante? Por construccion nuestro modelo con 199 variables ajustaria muy bien a nuestros datos, pero fallaria con datos nuevos por que se "aprendio de memoria" los 10,000 datos previos. 

Existe alguna manera de encontrar cuales de esas 199 son realmente las que importan para predecir nuestra variable target? Si, una manera es usar la tecnica de regularizacion, donde se le impone condiciones a los coeficientes _b_, asi un b muy chiquito 'casi 0' implicara que esa variable es poco importante. Mirando la ecuacion de arriba, el termino con beta casi cero no aporta casi nada.

Esta nocion de 'chico' no sera otra cosa que controlar el tamaño del vector _b_, esto es condicionar su **norma**.


### PCA y dimensionalidad

Cuando el dataset tiene muchas columnas, decenas, centenas o miles puede ser un problema. No solo por que el modelo esta mas predispuesto a aprenderse de memoria los datos y no asi a capturar el comportamiento real del sistema. Si no tambien es mas costoso computacionalmente, volvemos a la misma pregunta: como elegir entonces cuales columnas son relevantes o no? 

Los metodos para reducir de manera automatica el numero de columnas en un data set son llamados "metodos de reduccion de la dimensionalidad", el mas popular de todos es PCA o analisis de componentes principales.

La idea principal de PCA es **factorizar la matriz** de features X, y descomponerla en nuevas variables que capturen la mayor informacion posible del sistema, siendo entre ellas **linealmente independientes**. De esta manera uno aproxima el sistema con las variables que mayor informacion retenga, bajando asi la dimensionalidad. En nuestro ejemplo de 199 variables podemos elegir quedarnos con la cantidad necesaria para explicar el 70% de la variabilidad, esto dependiendo del problema puede ser 5, 10, 14, etc.


### Text-mining

Una parte muy importante de machine learning es NLP (procesamiento del lenguaje natural) donde es comun trabajar con matrices de alta dimensionalidad que representen las ocurrencias de palabras en los documentos que quieren analizar.

Imaginemos un dataset de mails, etiquetados como SPAM y NO SPAM, nuestro objetivo es poder predecir en funcion de las palabras contenidas en el subject del mail si es o no es SPAM.

```
Clase    Tema

SPAM     Ofertas de esta semana.    
CLEAN    Aviso de Transferencia.
SPAM     Pida su Prestamo.
SPAM     Descuento en pañales.
CLEAN    FWD: Fotos Cumpleaños
CLEAN    RE: Consulta Algebra lineal
SPAM     COTO TE CONOCE

````

Para estro primero tenemos que codificar nuestras features como lo hicimos para las variables categoricas, con esa misma idea ahora tendriamos una columna por cada palabra presente en TODO el dataset. Entonces cada mail sera representado por un vector larguisimo con ceros y unos dependiendo de las palabras presentes en el.

Veamos como ejemplo, la representacion de los primeros 2 mails:

``` 
Clase    ['Ofertas', 'de', 'esta', 'semana', 'Aviso',  'Transferencia', 'Pida', 'su', 'pañales',..., 'CONOCE']

SPAM     [1,          1,      1,        1,        0,                0,  ,   0,     0,          0, ...      0]
CLEAN    [0,          1,      0,        0,        1,                1,  ,   0,     0,          0, ...      0]

````

Como vemos, tamaño de esta matriz de features dependera de la cantidad de palabras presentes en los mails, mientras mas mails, mas palabras, mayor dimensionalidad. El diccionario de la RAE contiene 88.000 palabras...imaginen el tamaño de esa matriz si hablamos por ejemplo de una base de datos de 100,000 mails.

Sin embargo estas matrices tendran muchisimos 0, por lo que la informacion estara muy dispersa, a esto se lo llama **sparse matrix representation**. Para poder manejar este tipo de estructuras y reducirla es necesario usar  **factorizacion matricial**, como SVD o **singular-value decomposition** para poder reducir el tamaño concentrando toda la informacion. Con esta representacion vemos nuevamente como podemos usar todas las propiedades de operaciones vectoriales y matriciales para analizar texto maximizando la eficiencia del procesamiento, a esto se lo conoce como  **Latent Semantic Analysis**.

### Sistemas de recomendacion

Como hace tiktok para enviciarte con videitos hipnoticos uno detras de otro? como que que es tiktok? bueno bueno, youtube, netflix, spotify, el que quieras, como le hacen? En machine learning predecir que sugerencia va a tener mayor impacto se lo suele llamar "sistemas de recomendacion".

El ejemplo mas burdo es mercado libre, bombardeandote con ofertas similares a tu ultima busqueda, buscate una vez el precio de una lampara... listo para mercadolibre ahora queres poner un local de EL EMPORIO DE LAS LAMPARAS. Esta similaridad, en principio basica de simplemente repetir la ultima busqueda se puede mejorar si definimos mejor que es algo 'similar'.

Abstrayendonos como hace un rato donde cada mail era un vector, uno puede pensar en que cada producto de mercado libre es un vector y entonces podriamos ofrecer vectores que maximizen esta similitud con ultimo vector buscado. Para este tipo de problemas es mas util medir la similitud de vectores por **similitud coseno**.


Imagenemos los siguientes datos de nuestro almacen:

```
cliente    productos

user_1         1x pan,      1x salchica,    1x jugo
user_2       100x pan,  100x salchichas,  100x jugo
user_3         1x pan,       1x cerveza,    1x paty
```

La similitud coseno tiene en cuenta el angulo entre los vectores, la similitud euclidia (la norma de la que hablamos antes) nos mediria la distancia del vector diferencia. En este sentido, usando la similitud coseno, el cliente 1 es mas similar al cliente 2. Por otra parte, usando la similitud euclidia, el cliente 1 es mas similiar al cliente 3.


### Inteligencia Artificial

Inteligencia Artificial! Eso de lo que todo mundo habla y teme, que no es mas que un arreglo _profundo_ de redes neuronales. Debido a la evolucion del hardware es posible entrenar grandes volumenes de datos y muchas capas (por eso el deep). Los metodos de redes neuronales profundas o _Deep learning_ son el ultimo grito de la moda! (en la jerga se estila usar _state-of-the-art_ ) obteniendo permonfaces nunca antes alcanzadas en todo tipo de problemas, analisis de imagenes, videos, texto, audio.

En el fondo estas redes neuronales son representadas como matrices, vectores y tensores, donde un tensor no es otra cosa que una matriz en 3D (como lo vimos en el ejemplo de videos). Es por esto que algebra lineal es central para la descripcion de los metodos de deep learning, toda la documentacion de las implementaciones estan dadas en notacion tensorial, incluso una de las libreras mas populares se llama _TensorFlow_.


## Resumiendo:

Imagino que despues de todos estos ejemplos, estan convencidos de lo necesario que es saber algebra lineal. El lenguaje basico detras de todas las ideas de machine learning son los vectores matricas y transformaciones lineales. Sin esto, es muy dificil y menos provechosa cualquier texto o clase de ML. 

